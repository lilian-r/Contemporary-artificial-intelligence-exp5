{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "model = models.vgg16(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (pooling): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Linear(in_features=25088, out_features=4096, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(FeatureExtractor, self).__init__()\n",
    "    self.features = list(model.features)\n",
    "    self.features = nn.Sequential(*self.features)\n",
    "    self.pooling = model.avgpool\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = model.classifier[0]\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.features(x)\n",
    "    out = self.pooling(out)\n",
    "    out = self.flatten(out)\n",
    "    out = self.fc(out) \n",
    "    return out \n",
    " \n",
    "model = models.vgg16(pretrained=True)\n",
    "new_model = FeatureExtractor(model)\n",
    " \n",
    "device = torch.device(\"cpu\")\n",
    "new_model = new_model.to(device)\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    " \n",
    "transform = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.CenterCrop(512),\n",
    "  transforms.Resize(448),\n",
    "  transforms.ToTensor()                              \n",
    "])\n",
    " \n",
    "features = []\n",
    " \n",
    "f = open(\"train.txt\",\"r\")   #设置文件对象\n",
    "str = f.read().split('\\n')     #将txt文件的所有内容读入到字符串str中\n",
    "f.close()   #将文件关闭\n",
    "#print(len(str))\n",
    "train_label = []\n",
    "text_train = []\n",
    "image_train = []\n",
    "x_train = []\n",
    "for i in range(1, len(str)-1):\n",
    "  temp = str[i].split(\",\")\n",
    "  path = os.path.join('data', temp[0] + '.jpg')\n",
    "  img = cv2.imread(path)\n",
    "  img = transform(img)\n",
    "  img = img.reshape(1, 3, 448, 448)\n",
    "  img = img.to(device)\n",
    "  with torch.no_grad():\n",
    "    feature = new_model(img)\n",
    "  features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    " \n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = []\n",
    "\n",
    "f = open(\"test_without_label.txt\",\"r\")   #设置文件对象\n",
    "str = f.read().split('\\n')     #将txt文件的所有内容读入到字符串str中\n",
    "f.close()   #将文件关闭\n",
    "image_test = []\n",
    "for i in range(1, len(str)-1):\n",
    "    temp = str[i].split(\",\")\n",
    "    path = os.path.join('data', temp[0] + '.jpg')\n",
    "    img = cv2.imread(path)\n",
    "    img = transform(img)\n",
    "    img = img.reshape(1, 3, 448, 448)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        feature_test = new_model(img)\n",
    "    features_test.append(feature_test.cpu().detach().numpy().reshape(-1))\n",
    "    \n",
    "features_test = np.array(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('output.txt',features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_without_label.txt\",\"r\")   #设置文件对象\n",
    "str = f.read().split('\\n')     #将txt文件的所有内容读入到字符串str中\n",
    "f.close()   #将文件关闭\n",
    "test_x = []\n",
    "text_test = []\n",
    "image_test = []\n",
    "for i in range(1, len(str)-1):\n",
    "    temp = str[i].split(\",\")\n",
    "    # print(temp[0],temp[1])\n",
    "    test_x.append(temp[0])\n",
    "    text_path = 'data/' + temp[0] + '.txt'\n",
    "    text = open(text_path, 'r', encoding='gb18030')\n",
    "    text_test.append(text.read())\n",
    "    text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train.txt\",\"r\")   #设置文件对象\n",
    "str = f.read().split('\\n')     #将txt文件的所有内容读入到字符串str中\n",
    "f.close()   #将文件关闭\n",
    "#print(len(str))\n",
    "train_label = []\n",
    "text_train = []\n",
    "image_train = []\n",
    "x_train = []\n",
    "# high, width = 0, 0 \n",
    "for i in range(1, len(str)-1):\n",
    "    # x_train.append(temp[0])\n",
    "    temp = str[i].split(\",\")\n",
    "    #print(temp[0],temp[1])\n",
    "    train_label.append(temp[1])\n",
    "    text_path = 'data/' + temp[0] + '.txt'\n",
    "    text = open(text_path, 'r', encoding='gb18030')\n",
    "    text_train.append(text.read())\n",
    "    text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4511, 17154) (4000, 17154) (511, 17154)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "features_text = tfidf.fit_transform(text_train+text_test).toarray()\n",
    "\n",
    "text_train = features_text[:len(text_train)]\n",
    "text_test = features_text[-len(text_test):]\n",
    "\n",
    "print(features_text.shape, text_train.shape, text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 21250) (511, 21250)\n"
     ]
    }
   ],
   "source": [
    "image_train = features\n",
    "combined_x_train = np.concatenate((np.array(image_train), np.array(text_train)), axis=1)\n",
    "image_test = features_test\n",
    "combined_x_test = np.concatenate((np.array(image_test), np.array(text_test)), axis=1)\n",
    "\n",
    "print(combined_x_train.shape, combined_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "for i in range(len(train_label)):\n",
    "    train_label[i] = emotion_dict[train_label[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e8e6d003c29d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layer_acc = [0 for i in range(8)]\n",
    "# 将数据每次都随机划分，然后随即划分5次，看这五次的平均准确率\n",
    "for fold in range(2):\n",
    "    # 将训练数据划分为训练集和数据集\n",
    "    rand_factor = random.randrange(0,50) # 保证每一重验证时训练集和验证集划分的随机性\n",
    "    # 将测试集和验证集按照0.75:0.25的比例分配\n",
    "    # 尽量使验证集和测试集的大小相同\n",
    "    x_train, x_verify, y_train, y_verify = train_test_split(combined_x_train,train_label,test_size=0.25,random_state=rand_factor)\n",
    "    # 对几千个数据的大数据集solver‘adam'效果比较好\n",
    "    layer = [5, 10, 20, 30, 50, 100, 150, 200]\n",
    "    for i in range(8):\n",
    "        classifier = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(layer[i], ),max_iter = 10000, random_state=20)\n",
    "        fit = classifier.fit(x_train,y_train)\n",
    "        layer_acc[i] = layer_acc[i] + classifier.score(x_verify,y_verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer = 5  0.589\n",
      "layer = 10  0.626\n",
      "layer = 20  0.6074999999999999\n",
      "layer = 30  0.6205\n",
      "layer = 50  0.6165\n",
      "layer = 100  0.613\n",
      "layer = 150  0.6285000000000001\n",
      "layer = 200  0.613\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print('layer = ', end='')\n",
    "    print(layer[i],end='  ')\n",
    "    print(layer_acc[i]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-4f2938b3b650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'guid, tag'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# 调整好参数之后利用所有的训练数据再次对模型进行训练\n",
    "x_train, x_verify, y_train, y_verify = train_test_split(combined_x_train,train_label,test_size=0.25,random_state=rand_factor)\n",
    "classifier = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(150, ),max_iter = 10000, random_state=20)\n",
    "fit = classifier.fit(x_train,y_train)\n",
    "\n",
    "y_hat = classifier.predict(combined_x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 0 0 1 0 1 0 1 0 2 0 0 0 0 2 0 2 0 2 2 0 2 0 2 0 1 0 0 0 2 0 0 0 0 0\n",
      " 1 0 0 0 0 2 2 1 2 0 0 2 2 0 2 2 2 0 2 2 0 0 0 0 1 2 0 0 2 2 1 2 2 0 0 0 0\n",
      " 0 0 2 0 0 0 2 0 2 2 0 0 0 0 0 0 2 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n",
      " 0 0 2 0 2 0 0 0 0 2 0 0 2 0 0 0 0 0 2 2 2 2 1 0 0 0 2 0 1 0 2 0 0 0 0 0 2\n",
      " 0 2 0 2 0 0 0 1 2 1 0 0 1 0 0 2 0 0 0 1 0 1 2 2 2 2 0 0 0 1 2 2 0 0 0 0 1\n",
      " 2 0 0 0 2 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0\n",
      " 0 2 0 0 0 0 0 2 2 0 2 1 2 2 2 0 2 0 0 0 1 2 0 0 2 0 2 0 0 1 0 2 0 0 2 0 0\n",
      " 1 2 0 1 2 0 2 1 0 1 0 0 0 2 0 0 0 0 1 0 2 1 2 0 0 0 0 0 0 0 1 0 2 0 0 0 0\n",
      " 1 2 0 0 1 0 0 1 0 2 0 0 2 2 2 0 0 0 0 0 2 2 0 0 0 0 2 0 0 0 0 2 2 0 0 2 0\n",
      " 2 0 0 2 0 0 0 0 0 2 0 2 0 2 0 1 2 0 0 0 2 0 2 0 2 0 2 0 0 2 0 0 2 0 2 0 0\n",
      " 0 0 0 0 1 1 0 2 1 2 2 0 2 1 0 1 2 2 2 0 0 1 0 0 0 2 0 1 0 0 0 2 0 0 0 0 2\n",
      " 2 0 2 0 0 0 0 2 0 0 2 2 0 0 0 2 0 2 2 1 0 0 2 0 2 0 2 0 0 0 0 0 1 2 0 0 2\n",
      " 0 1 0 0 0 0 0 0 0 0 0 2 2 0 0 0 1 0 0 0 0 0 0 2 0 2 0 2 2 0 0 2 1 0 0 1 2\n",
      " 0 1 0 2 0 0 0 0 0 2 2 0 0 2 1 0 0 2 1 2 0 0 0 2 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_dict = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "# 写入结果文件\n",
    "f = open('test_without_label.txt',mode = 'w')\n",
    "y_pred = []\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i] == 0: y_pred.append('positive')\n",
    "    elif y_hat[i] == 1: y_pred.append('neutral')\n",
    "    elif y_hat[i] == 2: y_pred.append('negative')\n",
    "f.write( 'guid, tag' + '\\n')\n",
    "for i in range(len(y_hat)):\n",
    "    # print(type(i))\n",
    "    temp = y_pred[i]\n",
    "    f.write(test_x[i] + ',' + temp + '\\n')\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "消融模型 只用图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layer_acc = [0 for i in range(8)]\n",
    "# 将数据每次都随机划分，然后随即划分5次，看这五次的平均准确率\n",
    "for fold in range(2):\n",
    "    # 将训练数据划分为训练集和数据集\n",
    "    rand_factor = random.randrange(0,50) # 保证每一重验证时训练集和验证集划分的随机性\n",
    "    # 将测试集和验证集按照0.75:0.25的比例分配\n",
    "    # 尽量使验证集和测试集的大小相同\n",
    "    x_train, x_verify, y_train, y_verify = train_test_split(np.array(image_train),train_label,test_size=0.25,random_state=rand_factor)\n",
    "    # 对几千个数据的大数据集solver‘adam'效果比较好\n",
    "    layer = [5, 10, 20, 30, 50, 100, 150, 200]\n",
    "    for i in range(8):\n",
    "        classifier = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(layer[i], ),max_iter = 10000, random_state=20)\n",
    "        fit = classifier.fit(x_train,y_train)\n",
    "        layer_acc[i] = layer_acc[i] + classifier.score(x_verify,y_verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer = 5  0.562\n",
      "layer = 10  0.5854999999999999\n",
      "layer = 20  0.571\n",
      "layer = 30  0.579\n",
      "layer = 50  0.5605\n",
      "layer = 100  0.59\n",
      "layer = 150  0.586\n",
      "layer = 200  0.589\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print('layer = ', end='')\n",
    "    print(layer[i],end='  ')\n",
    "    print(layer_acc[i]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "消融模型 只用文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layer_acc = [0 for i in range(8)]\n",
    "# 将数据每次都随机划分，然后随即划分5次，看这五次的平均准确率\n",
    "for fold in range(2):\n",
    "    # 将训练数据划分为训练集和数据集\n",
    "    rand_factor = random.randrange(0,50) # 保证每一重验证时训练集和验证集划分的随机性\n",
    "    # 将测试集和验证集按照0.75:0.25的比例分配\n",
    "    # 尽量使验证集和测试集的大小相同\n",
    "    x_train, x_verify, y_train, y_verify = train_test_split(np.array(text_train),train_label,test_size=0.25,random_state=rand_factor)\n",
    "    # 对几千个数据的大数据集solver‘adam'效果比较好\n",
    "    layer = [5, 10, 20, 30, 50, 100, 150, 200]\n",
    "    for i in range(8):\n",
    "        classifier = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(layer[i], ),max_iter = 10000, random_state=20)\n",
    "        fit = classifier.fit(x_train,y_train)\n",
    "        layer_acc[i] = layer_acc[i] + classifier.score(x_verify,y_verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer = 5  0.6515\n",
      "layer = 10  0.654\n",
      "layer = 20  0.6495\n",
      "layer = 30  0.6465000000000001\n",
      "layer = 50  0.6545000000000001\n",
      "layer = 100  0.6425000000000001\n",
      "layer = 150  0.644\n",
      "layer = 200  0.6435\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print('layer = ', end='')\n",
    "    print(layer[i],end='  ')\n",
    "    print(layer_acc[i]/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1a5623649cdd12fac53e8731494a515685b0f017efcda3428947771dbc589d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
